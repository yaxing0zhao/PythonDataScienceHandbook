# Notes on Python Data Science Handbook

[Python Data Science Handbook](https://github.com/yaxing0zhao/PythonDataScienceHandbook/tree/master/notebooks)

## ipython

### 01:00
ipython, jupyter

### 01:01
`sum?`

### 01:02
shortcuts: ctrl+l:Clear terminal screen

### 01:03
**magic commands**: `%paste, %run pyscirpt.py, %timeit, %lsmagic `

### 01:04
`%history`

### 01:05
`!`

### 01:06
`%xmode, %debug, %pdb`

### 01:07

- `%time`: Time the execution of a single statement

- `%timeit`: Time repeated execution of a single statement for more accuracy

- `%prun`: Run code with the profiler

- `%lprun`: Run code with the line-by-line profiler

- `%memit`: Measure the memory use of a single statement

- `%mprun`: Run code with the line-by-line memory profiler

  

## NumPy

### 02:01
æ‰¹é‡åˆ›å»º**array**

### 02:02
 arrayæ“ä½œï¼Œå­é›†ï¼Œ`copyï¼Œreshapeï¼Œconcatenate (cbind, rbind, append), splitting: vsplit, hsplit.`

### 02:03
NumPy ä¸­å‘é‡åŒ–æ“ä½œï¼Œå„ç§æ•°å­¦è¿ç®—ï¼Œå‡å°‘ä½¿ç”¨functionså’Œloopã€‚

### 02:04
ç»Ÿè®¡è¿ç®—ï¼ŒNumPyä¸­å‡½æ•°æ¯”pythonè‡ªå¸¦å‡½æ•°è¦å¿«ï¼arrayä¸Šçš„ç»Ÿè®¡è¿ç®—ã€‚

### 02:05
arrayå‘é‡é—´æ“ä½œã€‚

### 02:06
arrayé€»è¾‘æ“ä½œï¼Œæ˜¯å¦ï¼Œæ¯”è¾ƒ

### 02:07
fancy index: æ‰¹é‡é€‰å–å­é›† `list[idx], array[row_idx, col_idx]`, å„ç§é€‰å–æ–¹æ³•â€¦â€¦

### 02:08
sorting arrayï¼šæ’åºåŠéƒ¨åˆ†æ’åº

### 02:09
structured array: similar to dataframe in pandas



## Pandas

### 03:01 
**series and dataframe** åˆ›å»º, index, `df1.shape()`(dim(df1))

### 03:02
series and dataframe å­é›†é€‰å–`df1.iloc[:nrow, :ncol], df1[colname]`

### 03:03
dataframe æ•°å­¦è¿ç®—ï¼Œè‡ªåŠ¨å¯¹é½æˆ–äº§ç”ŸNA, `a.add(b, fill_value=fill)` ã€‚

### 03:04
missing value: `np.nan(),df1.isnull(),df1.notnull(), df1.ffill(), df1.bfill(),df1.droupna(axis = 'colums')`

### 03:05
multi-index: dataframe ğŸ”›series by `stack()`, multi-index dataframe å–å­é›†æ—¶index must be sorted. `reset_index() `ç›¸å½“äºmelt: multi-index to index, `reverse is set_index()`.

### 03:06
df combination: np.concatenate( see 02:02), `pd.concat(df1, df2)`: å‚æ•°é€‰é¡¹ç±»ä¼¼mySQLã€‚`df1.append(df2)`ç±»ä¼¼rbind()

### 03:07
merge and join: `pd.merge(df1,df2, left_on='keyword1', right_on='keyword2', on='keyword3', suffixes=["_L", "_R"])'), df1.join(df2)` ç›¸å½“äºSQLä¸­çš„joinã€‚

### 03:08
groupby: `df1.mean(axis='columns')`, `df1.describe() `,`df1.groupby('key').sum(), df1.groupby('key').aggregate(['min', np.median, max]), df1.groupby('key').apply(fun),`,  `filter(), transform() and apply()`:ç±»ä¼¼apply in R. `df1.groupby(mapping).sum()`: replace a_idx with b_idx.

### 03:09
pivot table: å¤šå˜é‡groupby çš„ç®€åŒ–ç‰ˆã€‚`df1.pivot_table(data, values=None, index=None, columns=None, aggfunc={'survived':sum, 'fare':'mean'}, fill_value=None, margins=False, dropna=True, margins_name='All')` `pivot_table(), pd.cut()`, ***åº”ç”¨å®ä¾‹***ã€‚

### 03:10
stringï¼šæ–‡æœ¬æ“ä½œå’ŒRç±»ä¼¼åŠŸèƒ½ï¼Œ`mystr.str.len()` ***åº”ç”¨å®ä¾‹***ã€‚

### 03:11
time; pd.datatime and pd.datautil, pd.tseries.offsets module. Np.array(), ***åº”ç”¨å®ä¾‹***ã€‚

### 03:12
**high-performance**: numexpr.evaluate() for compound expression, `pd.eval()` is faster. `Query()` for filter operation.



## Matplotlib

### 04:01
matplotlib: style, `put.show() `åªåœ¨æœ«å°¾ä¸€æ¬¡å°±å¥½ã€‚`fig.save fig(filename.png).` å¤šå›¾å¸ƒå±€ã€‚

### 04:02
lineplot: åæ ‡è½´ï¼Œå›¾ä¾‹

### 04:03
scatter plot: å¤§æ•°æ®é¦–é€‰`plt.plot()`.

### 04:04
error bar: åŒ…æ‹¬è¿ç»­errorã€‚

### 04:05
å¯†åº¦å›¾å’Œè½®å»“å›¾ã€‚

### 04:10
ä¿®æ”¹åæ ‡è½´

### 04:14
**seaborn** ä¸»é¢˜ç”»å›¾æ›´ç¾è§‚ï¼Œç”¨å®ƒï¼



## ML

### 05:02
array: sample * feature. Target col is the predicted class/feature. **MLçš„åŸºæœ¬æ­¥éª¤**ï¼šimport/instantiate/fit/predict. Linear regression, GaussianNB, across-validation, PCA, GMM cluster, hand-written digits, Isomap (dimensionality reduction) å®ä¾‹ã€‚

### 05:03
**cross-validation**ï¼Œ validation-curve, learning_curve

### 05:04
feature engineering: categorical, text, image, derived features, missing value imputation.

### 05:05
naive bayes: multi-NB

### 05:06
linear regression, regularization

### 05:07
SVA

### 05:08
RF

### 05:09
PCA: tends to be highly affected by outliers, `RandomizedPCA` and `SparsePCA`

### 05:10
manifold learning: multidimensional scaling (MDS), locally linear embedding (LLE), and isometric mapping (IsoMap).

### 05:11
k-means

### 05:12
gaussian mixtures

### 05:13
kernel density estimation

### 05:14
image features
